# **Лабораторная работа №10. Алгоритм k ближайших соседей**

> **Цель работы**: Познакомиться с алгоритмом **k ближайших соседей (k-NN)** — одним из простейших алгоритмов машинного обучения, научиться применять его для классификации и регрессии, реализовать шаги: вычисление расстояния, выбор соседей, принятие решения, протестировать на простых данных.

---

## **Теория**

В главе 10 книги *"Грокаем алгоритмы"* рассматривается **алгоритм k ближайших соседей (k-Nearest Neighbors, k-NN)** — метод обучения с учителем, используемый для **классификации** и **регрессии**.

### Как работает k-NN:
1. Каждый объект представляется точкой в **n-мерном пространстве** (по признакам).
2. Для нового объекта:
   - Вычисляется расстояние до всех известных точек.
   - Выбираются **k ближайших** соседей.
   - **Классификация**: назначается класс по **большинству голосов** среди соседей.
   - **Регрессия**: предсказывается **среднее значение** целевой переменной соседей.

### Расстояние:
- Часто используется **евклидово расстояние**:
  \[
  d = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \dots + (x_n - y_n)^2}
  \]

### Выбор k:
- Малое k (например, 1) — чувствителен к шуму.
- Большое k — сглаживает результат, но может потерять локальные особенности.

### Преимущества:
- Простота понимания и реализации.
- Не делает предположений о данных.

### Недостатки:
- Медленно работает на больших данных (нужно сравнивать со всеми).
- Чувствителен к масштабу признаков.

---

## **Практическая часть**

### 1. Создайте рабочий файл
- Создайте папку `lab10_knn`.
- Внутри — файл `knn.py`.
- Откройте его в редакторе.

### 2. Подготовьте данные: фрукты по размеру и текстуре
Создадим простой набор данных: яблоки и апельсины по весу (в граммах) и текстуре (0 — гладкий, 1 — шершавый):

```python
data = [
    [150, 0, 'яблоко'],   # вес, текстура, класс
    [170, 0, 'яблоко'],
    [140, 0, 'яблоко'],
    [130, 1, 'апельсин'],
    [160, 1, 'апельсин'],
    [155, 1, 'апельсин']
]

# Новый фрукт: нужно классифицировать
new_fruit = [152, 0]  # вес 152 г, гладкий
```

### 3. Реализуйте евклидово расстояние
```python
import math

def euclidean_distance(point1, point2):
    return math.sqrt(sum((a - b) ** 2 for a, b in zip(point1, point2)))
```

Тест:
```python
print(euclidean_distance([1, 2], [4, 6]))  # должно быть 5.0
```

### 4. Вычислите расстояния до всех точек
```python
distances = []
for row in data:
    dist = euclidean_distance(new_fruit, row[:2])  # только признаки
    distances.append((dist, row[2]))  # (расстояние, класс)
```

### 5. Отсортируйте по расстоянию и выберите k соседей
```python
k = 3
distances.sort(key=lambda x: x[0])
k_neighbors = distances[:k]

print("Ближайшие соседи:")
for d, cls in k_neighbors:
    print(f"Расстояние: {d:.2f}, Класс: {cls}")
```

### 6. Проголосуйте за класс
```python
from collections import Counter

classes = [cls for _, cls in k_neighbors]
vote = Counter(classes).most_common(1)[0][0]

print(f"Предсказанный класс: {vote}")
```

### 7. Обобщите в функцию `knn_predict`
```python
def knn_predict(data, new_point, k=3):
    # Вычисляем расстояния
    distances = []
    for row in data:
        dist = euclidean_distance(new_point, row[:-1])
        distances.append((dist, row[-1]))
    # Сортируем и выбираем k
    distances.sort(key=lambda x: x[0])
    k_classes = [cls for _, cls in distances[:k]]
    # Голосование
    return Counter(k_classes).most_common(1)[0][0]

# Тест
print("k-NN предсказывает:", knn_predict(data, new_fruit, k=3))
```

### 8. Примените к регрессии: предсказание цены
Теперь используем k-NN для **регрессии** — предсказания числового значения.

Данные: площадь дома (м²) и цена (тыс. руб):
```python
houses = [
    [80, 8000],
    [90, 9000],
    [100, 10000],
    [70, 7000],
    [110, 11000]
]

def knn_regression(data, new_point, k=3):
    distances = []
    for row in data:
        dist = euclidean_distance([row[0]], [new_point])
        distances.append((dist, row[1]))
    distances.sort(key=lambda x: x[0])
    k_values = [val for _, val in distances[:k]]
    return sum(k_values) / len(k_values)  # среднее

print("Предсказанная цена за дом 85 м²:", knn_regression(houses, 85, k=3))
```

---

## **Самостоятельная часть**

### 1. Нормализация признаков
Добавьте в данные фрукт с весом в **килограммах**:
```python
data_mixed = [
    [0.150, 0, 'яблоко'],
    [130, 1, 'апельсин']  # в граммах!
]
```
Попробуйте классифицировать `[0.152, 0]` — результат будет некорректным из-за разного масштаба.

Напишите функцию `normalize(data)`, которая:
- Нормализует каждый признак: `(x - min) / (max - min)`
- Возвращает нормализованные данные.

Примените нормализацию перед k-NN.

### 2. Выбор оптимального k
Протестируйте `k` от 1 до 5 на исходных данных.  
При каком `k` предсказание наиболее устойчиво?  
Сделайте вывод: как выбор `k` влияет на результат.

---

## **Контрольные вопросы** (ответы устно)

1. В каких задачах можно использовать k-NN? Почему он называется «ленивым» алгоритмом?  
2. Почему важно нормализовать признаки перед применением k-NN?

---

## **Слепая печать**
1. Пройдите два урока слепой печати на [Python](https://stamina-online.com/ru/workout/programming/15)

---

## **Критерии оценивания**

| Часть работы              | Оценка | Комментарий |
|--------------------------|--------|-------------|
| Практическая часть       | 3      | Все шаги выполнены, классификация и регрессия работают |
| Слепая печать            | 4      | Подтверждено выполнение |
| Самостоятельная часть    | 5      | Задачи решены, особенно нормализация — показывает понимание важности подготовки данных |
